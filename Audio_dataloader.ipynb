{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LunarLynxLima/Stethoscope-AI/blob/main/Audio_dataloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kc6yhfoT6IG",
        "outputId": "3a5ffc8c-6e6f-4360-a343-212e9859efe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHjkwbibXLCo",
        "outputId": "d6e20672-82d2-4f72-c4b6-06731df11182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "import torch         # 2.0.1+cpu\n",
        "import torchaudio    # 2.0.2+cpu\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "import re\n",
        "# import tempfile\n",
        "import matplotlib.pyplot as plt\n",
        "from pydub import AudioSegment\n",
        "\n",
        "import warnings\n",
        "# warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Z0ABUxkCTvGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def instance_uniform_length(waveform,sample_rate,desired_duration_seconds : int = 10,add_wav_type = 1, verbose : int  = 0):\n",
        "    def clip_wav(waveform, sample_rate,desired_duration_samples):\n",
        "        clipped_waveform = waveform[:, :desired_duration_samples]\n",
        "        return clipped_waveform, sample_rate\n",
        "\n",
        "    def add_wav(waveform, sample_rate,desired_duration_samples,silence = 1):\n",
        "        \"\"\"\n",
        "        silence = 0 : no wave/silence concat;\n",
        "        silence = -1 : ones concat ;\n",
        "        silence = 1 : replay concat\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculates the amount of silence needed\n",
        "        # silence_samples = desired_duration_samples - len(waveform)/sample_rate\n",
        "        silence_samples = desired_duration_samples - waveform.shape[1]\n",
        "        if silence == 0 :\n",
        "            # null silence waveform\n",
        "            silence_waveform = torch.zeros((waveform.shape[1], silence_samples))\n",
        "            # Concatenate original + silence\n",
        "            extended_waveform = torch.cat([waveform, silence_waveform], dim=1)\n",
        "        elif silence == -1:\n",
        "            # white noise silence waveform\n",
        "            silence_waveform = torch.ones((waveform.shape[1], silence_samples))\n",
        "            # Concatenate original + silence\n",
        "            extended_waveform = torch.cat([waveform, silence_waveform], dim=1)\n",
        "        elif silence == 1:\n",
        "            # replay silence waveform\n",
        "          rest_waveform,sample_rate = clip_wav(waveform, sample_rate,silence_samples)\n",
        "          extended_waveform = torch.cat([waveform, rest_waveform], dim=1)\n",
        "        else:\n",
        "          return -1,-1\n",
        "        return extended_waveform,sample_rate\n",
        "\n",
        "    desired_duration_samples = int(sample_rate * desired_duration_seconds)\n",
        "\n",
        "    # Check if the input audio is shorter than the desired duration\n",
        "    if waveform.shape[1] < desired_duration_samples:\n",
        "        processed_waveform,sample_rate = add_wav(waveform, sample_rate,desired_duration_samples,silence=add_wav_type)\n",
        "        if verbose : print(f\"type of concatenated audio is {add_wav_type}\")\n",
        "    else:\n",
        "        processed_waveform,sample_rate = clip_wav(waveform, sample_rate,desired_duration_samples)\n",
        "        if verbose : print(f\"Audio Cliped\")\n",
        "    return processed_waveform,sample_rate\n",
        "\n",
        "def instance_uniform_sample_rate(waveform, original_sample_rate,target_sample_rate  : int = 7000,verbose:int = 0):\n",
        "    if verbose : print(f\"{original_sample_rate} to {target_sample_rate}\")\n",
        "    if verbose:\n",
        "      print(type(waveform))\n",
        "      print(f\"Waveform: {waveform}\")\n",
        "    resampled_waveform = torchaudio.transforms.Resample(original_sample_rate, target_sample_rate)(waveform)\n",
        "    return resampled_waveform,target_sample_rate\n",
        "\n",
        "def instance_uniform_amplitude(file_path : str, req_uniform_amplitude :int = -20, verbose : int = 0):\n",
        "    target_amplitude = req_uniform_amplitude  # In dBFS (decibels relative to full scale)\n",
        "    if file_path.endswith(\".wav\"):\n",
        "      audio = AudioSegment.from_mp3(file_path) # AudioSegment.from_mp3 is a method in the pydub library that reads an MP3 file and returns an AudioSegment object.\n",
        "      normalized_audio = audio.apply_gain(target_amplitude - audio.dBFS)\n",
        "      #the apply_gain function in the pydub library is used to apply a gain (amplification) to an audio segment.\n",
        "      #We can use the apply_gain method to increase or decrease the volume of the audio by a specified amount in decibels (dB)\n",
        "      waveform = [normalized_audio.get_array_of_samples()]\n",
        "      waveform = torch.tensor(waveform)\n",
        "\n",
        "      sample_rate = normalized_audio.frame_rate\n",
        "      return waveform, sample_rate\n",
        "    return -1,-1\n",
        "\n",
        "def normalize_wav_path(file_path: str, uniform_amplitude: bool = True, uniform_length: bool = True, req_wav_length: int = 10, add_wav_type:int=1, uniform_sample_rate: bool = True, req_sample_rate: int = 7000, verbose: int = 0):\n",
        "    \"\"\"\n",
        "    Normalize an audio file given its path by applying optional processing steps.\n",
        "\n",
        "    Parameters:\n",
        "    - file_path: The path to the input audio file.\n",
        "    - uniform_amplitude: If True, normalize the audio to a uniform amplitude (default is True).\n",
        "    - uniform_length: If True, adjust the audio duration to a specified length (default is True).\n",
        "    - req_wav_length: The desired duration in seconds (default is 10 seconds).\n",
        "    - uniform_sample_rate: If True, set the audio sample rate to a uniform value (default is True).\n",
        "    - req_sample_rate: The target sample rate (default is 7000 Hz).\n",
        "    - verbose: Verbosity level (0 for no output, 1 for basic output, default is 0).\n",
        "\n",
        "    Returns:\n",
        "    - processed_waveform: The normalized audio waveform. : <class 'torch.Tensor'> == e.g. torch.Size([1, 210000])\n",
        "    - processed_sample_rate: The sample rate of the normalized audio. <class 'int'>\n",
        "\n",
        "    If the input file is empty or any processing step results in an issue, the function returns (-1, -1).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if os.path.getsize(file_path) > 0:  # Check if the input audio file is not empty\n",
        "\n",
        "        # Load the waveform and sample rate from the audio file\n",
        "        waveform, sample_rate = torchaudio.load(file_path)\n",
        "        processed_waveform, processed_sample_rate = waveform, sample_rate\n",
        "\n",
        "        # Apply optional processing steps\n",
        "        if uniform_amplitude:\n",
        "            # Normalize amplitude and convert the waveform to a floating-point type\n",
        "            import torch\n",
        "            processed_waveform, processed_sample_rate = instance_uniform_amplitude(file_path, req_uniform_amplitude = -20, verbose=verbose)\n",
        "            processed_waveform = processed_waveform.to(torch.float32)\n",
        "\n",
        "        if uniform_length:\n",
        "            # Adjust audio duration to the desired length\n",
        "            processed_waveform, processed_sample_rate = instance_uniform_length(processed_waveform, processed_sample_rate, desired_duration_seconds=req_wav_length, add_wav_type=add_wav_type, verbose=verbose)\n",
        "\n",
        "        if uniform_sample_rate:\n",
        "            # Set the sample rate to the desired value\n",
        "            processed_waveform, processed_sample_rate = instance_uniform_sample_rate(processed_waveform, processed_sample_rate, target_sample_rate=req_sample_rate, verbose=verbose)\n",
        "\n",
        "        return processed_waveform, processed_sample_rate\n",
        "\n",
        "    else:\n",
        "        # If the input audio file is empty, return a sentinel value\n",
        "        if verbose:\n",
        "            print(\"File is empty\")\n",
        "        return -1, -1"
      ],
      "metadata": {
        "id": "WlkXdINoOkws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mel-Spectogram is preferred in literature, therefore defualt\n",
        "\n",
        "def spectrogram_feature_extraction(waveform, SAMPLE_RATE, _n_lfcc : int = 40, _n_mfcc : int = 512,type : int = 0, verbose = 0):\n",
        "    # waveform, SAMPLE_RATE = torchaudio.load(instance_file_path)\n",
        "    if verbose : print(f'waveform shape: {waveform.shape}; SAMPLE_RATE: {SAMPLE_RATE}')\n",
        "\n",
        "    # Define transform\n",
        "    if(type == 0) : spectrogram = T.MelSpectrogram(sample_rate=SAMPLE_RATE)  # Mel-scale Spectrogram\n",
        "    if(type == 1) : spectrogram = T.Spectrogram(power=None)  # Raw Spectrogram (complex-valued)\n",
        "    if(type == 2) : spectrogram = T.Spectrogram(power=2)  # Power Spectrogram (real-valued)\n",
        "    if(type == 3) : spectrogram = T.LFCC(sample_rate=SAMPLE_RATE, n_lfcc=_n_lfcc)  #(n_lfcc -> parameter) Linear Frequency Cepstral Coefficient\n",
        "    if(type == 4) : spectrogram = T.MFCC(sample_rate=SAMPLE_RATE, n_mfcc=_n_mfcc)  #(n_fft -> parameter) Mel Frequency Cepstral Coefficient\n",
        "    if(type == 5) :\n",
        "        # Compute the zero-crossing rate\n",
        "        zero_crossing_rate = torchaudio.transforms.ZeroCrossingRate()(waveform)\n",
        "        # Calculate the mean zero-crossing rate\n",
        "        mean_zero = zero_crossing_rate.mean(dim=1)\n",
        "        mean_zero = mean_zero.numpy()\n",
        "\n",
        "        return mean_zero\n",
        "    if(type == 6) :\n",
        "        # Compute the Short-Time Fourier Transform (STFT)\n",
        "        spectrogram = T.Spectrogram()(waveform)\n",
        "        # Compute the chroma feature from the STFT\n",
        "        chroma = T.Chroma()(spectrogram)\n",
        "        # Calculate the mean along the time axis\n",
        "        chroma_mean = chroma.mean(dim=2)\n",
        "        # Convert the result to a NumPy array if needed\n",
        "        chroma_mean = chroma_mean.numpy()\n",
        "\n",
        "        return chroma\n",
        "    if(type == 7) :\n",
        "        # Calculate the RMS using torchaudio's rms function\n",
        "        rms = torchaudio.transforms.RMS()(waveform)\n",
        "        # Calculate the mean along the time axis\n",
        "        rms_mean = rms.mean(dim=1)\n",
        "\n",
        "        return rms_mean\n",
        "    # Perform transform\n",
        "    spec = spectrogram(waveform)\n",
        "    return spec"
      ],
      "metadata": {
        "id": "ztVDewneQ77r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processed_waveform, processed_sample_rate =(normalize_wav_path(r\"Dataset_1\\audio_and_txt_files\\101_1b1_Al_sc_Meditron.wav\"))\n",
        "# # print(processed_waveform.shape)\n",
        "# # print(processed_sample_rate)\n",
        "# # print(processed_waveform)\n",
        "# audio_features_extraction = spectrogram_feature_extraction(waveform=processed_waveform, SAMPLE_RATE=processed_sample_rate, _n_lfcc = 40, _n_mfcc  = 512,type = 0, verbose = 0).flatten()\n",
        "# print(len(audio_features_extraction))"
      ],
      "metadata": {
        "id": "TCvgE76kSb4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_no2bjG7Y8J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "import wave\n",
        "import contextlib\n",
        "\n",
        "class AudioDataloader():\n",
        "    def __init__(self, annotated_file, features_list, choice, transform=None, target_transform = None):\n",
        "        dataset = pd.read_csv(annotated_file)\n",
        "        self.df = pd.DataFrame(dataset)\n",
        "        self.df1 = self.df[features_list]\n",
        "        self.audio_files = self.df['Path']\n",
        "        self.audio_labels = self.df['Diagnosis']\n",
        "        self.choice = choice\n",
        "        self.annotations = features_list\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # waveform, sample_rate = torchaudio.load(self.audio_files[idx]) # torchaudio.load() returns a tensor of float type and shape of [channel, time]\n",
        "        # print('Audio Tensor: ', waveform)\n",
        "        # print('Size of the tensor:', waveform.size())\n",
        "\n",
        "        normalised_waveform, normalised_sample_rate = normalize_wav_path(self.audio_files[idx], verbose = 0)\n",
        "        # print(normalised_waveform.size())\n",
        "        audio_features_extracted = spectrogram_feature_extraction(waveform=normalised_waveform, SAMPLE_RATE = normalised_sample_rate).flatten()\n",
        "        print(audio_features_extracted.size())\n",
        "\n",
        "        # audio_features_extracted = torch.audio_features_extracted[60000]\n",
        "        # print('Audio_features_shape', audio_features_extracted.shape)\n",
        "        # print('Shape: ', audio_features_extracted.size())\n",
        "        # print('Normalised Waveform- ', normalised_waveform)\n",
        "        # print('Normalised SR', normalised_sample_rate)\n",
        "        # print()\n",
        "\n",
        "        # clipped_waveform = waveform[:, :10] #depends on how much duration\n",
        "        # normalise_waveform = torch.nn.functional.normalize(clipped_waveform, dim=1) #amplitude\n",
        "\n",
        "        # self.normalize = T.Resample(orig_freq=44100, new_freq=16000) #frequency same normalized_waveform)\n",
        "\n",
        "        #For time duration\n",
        "        # with contextlib.closing(wave.open(self.audio_files[idx],'r')) as f:\n",
        "        #   frames = f.getnframes()\n",
        "        #   rate = f.getframerate()\n",
        "        #   duration = frames / float(rate)\n",
        "        #   print('Time Duration of the audio file: ', duration)\n",
        "\n",
        "        # print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
        "        # print()\n",
        "\n",
        "        encodings = {'URTI':1, 'Normal':2, 'Asthma':3, 'COPD': 4, 'LRTI': 5, 'Bronchiectasis':6, 'Pneumonia':7, 'Bronchiolitis':8, 'Heart Failure + Lung Fibrosis':9,\n",
        "                     'Heart Failure + COPD':10, 'Pleural Effusion':11, 'Heart Failure':12, 'Asthma + Lung Fibrosis':13, 'BRON':14, 'Lung Fibrosis':15, 'DAS':16,\n",
        "                     'CAS & DAS':17, 'CAS':18}\n",
        "        label = 0\n",
        "        if self.target_transform == True:\n",
        "          if(self.choice ==1): #binary classification\n",
        "            if(self.audio_labels[idx]=='Normal'):\n",
        "              label = 0\n",
        "            else:\n",
        "              label = 1\n",
        "          else:   #multi-class classification\n",
        "            label =  encodings[self.audio_labels[idx]]\n",
        "\n",
        "        return self.annotations, audio_features_extracted, label"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, default_collate\n",
        "import sys\n",
        "\n",
        "print('Pick the features that you want to include in the dataset: \\n')\n",
        "print('1. Location\\n')\n",
        "print('2. Acoustic Channel\\n')\n",
        "print('3. Equipment\\n')\n",
        "print('4. Diagnosis\\n')\n",
        "print('5. Age\\n')\n",
        "print('6. Gender\\n')\n",
        "print('7. Sound Type\\n')\n",
        "print('8. Chest Zone\\n')\n",
        "\n",
        "# line = [int(x) for x in input().split()]\n",
        "# list_set = set(line)\n",
        "# unique_feature_codes = (list(list_set))\n",
        "features_names = ['Location', 'Acoustic Channel', 'Equipment', 'Diagnosis', 'Age', 'Gender', 'Sound Type', 'Chest Zone']\n",
        "\n",
        "features = []\n",
        "features.append('Patient ID')\n",
        "features.append('Path')\n",
        "\n",
        "#added\n",
        "features.append('Diagnosis')\n",
        "features.append('Equipment')\n",
        "\n",
        "# for i in range(len(unique_feature_codes)):\n",
        "#   features.append(features_names[unique_feature_codes[i]])\n",
        "\n",
        "# set up DataLoader for training set\n",
        "dataset = AudioDataloader(r'/content/drive/MyDrive/Stethoscope Dataset/mylist.csv',features,1, False, True)\n",
        "trainset, testset = random_split(dataset, [0.7, 0.3])\n",
        "\n",
        "loader = DataLoader(dataset, shuffle=True, batch_size=4)"
      ],
      "metadata": {
        "id": "HMS5DiIA7q1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6639fdb-dc72-49a3-bdc5-94ba0e559b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pick the features that you want to include in the dataset: \n",
            "\n",
            "1. Location\n",
            "\n",
            "2. Acoustic Channel\n",
            "\n",
            "3. Equipment\n",
            "\n",
            "4. Diagnosis\n",
            "\n",
            "5. Age\n",
            "\n",
            "6. Gender\n",
            "\n",
            "7. Sound Type\n",
            "\n",
            "8. Chest Zone\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in loader:\n",
        "    batch_annotation, audio_batch, label_batch = batch"
      ],
      "metadata": {
        "id": "GktpLQa_diHK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "51ed7bc7-9c1a-463e-b1cb-7f27c8eec051"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n",
            "torch.Size([44928])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-952427c1f126>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mbatch_annotation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-744ae469b793>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print('Size of the tensor:', waveform.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mnormalised_waveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalised_sample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_wav_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# print(normalised_waveform.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0maudio_features_extracted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectrogram_feature_extraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalised_waveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalised_sample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-ab20d4be7dbc>\u001b[0m in \u001b[0;36mnormalize_wav_path\u001b[0;34m(file_path, uniform_amplitude, uniform_length, req_wav_length, add_wav_type, uniform_sample_rate, req_sample_rate, verbose)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Normalize amplitude and convert the waveform to a floating-point type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mprocessed_waveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_sample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance_uniform_amplitude\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_uniform_amplitude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mprocessed_waveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_waveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-ab20d4be7dbc>\u001b[0m in \u001b[0;36minstance_uniform_amplitude\u001b[0;34m(file_path, req_uniform_amplitude, verbose)\u001b[0m\n\u001b[1;32m     59\u001b[0m       \u001b[0;31m#We can use the apply_gain method to increase or decrease the volume of the audio by a specified amount in decibels (dB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m       \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnormalized_audio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_array_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m       \u001b[0mwaveform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_audio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(44928, 1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1024, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 1\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for batch in loader:\n",
        "        audio_annotation, audio_batch, label_batch = batch\n",
        "        # print(audio_batch.shape)\n",
        "        # print('Label batch', label_batch)\n",
        "        y_pred = model(audio_batch) #16 X 768  4X4\n",
        "\n",
        "        y_pred = y_pred.reshape([4,1]).to(torch.float).flatten()\n",
        "        label_batch = label_batch.reshape([4,1]).to(torch.float).flatten()\n",
        "        # print('y_pred', y_pred)\n",
        "\n",
        "        loss = loss_fn(y_pred, label_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# create one test tensor from the testset\n",
        "annotated_data, X_test, y_test = default_collate(testset)\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "acc = (y_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))"
      ],
      "metadata": {
        "id": "oftqc9uVPFkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "n5yJ4hkvX-NZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/drive/MyDrive/Stethoscope Dataset/Database(1)/audio_and_txt_files/101_1b1_Al_sc_Meditron.txt\n",
        "\n",
        "# /content/drive/MyDrive/Stethoscope Dataset/Database(2)/Audio Files/BP100_N,N,P R M,70,F.wav\n",
        "\n",
        "# /content/drive/MyDrive/Stethoscope Dataset/Database(3)/train_wav/40069321_15.3_0_p1_981.wav"
      ],
      "metadata": {
        "id": "yjtz_dqP7q8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = AudioDataloader(r'/content/drive/MyDrive/Stethoscope Dataset/mylist.csv',features, 2, False, True)\n",
        "trainset, testset = random_split(dataset, [0.7, 0.3])\n",
        "loader = DataLoader(dataset, shuffle=True, batch_size=4)"
      ],
      "metadata": {
        "id": "da6MilC81cv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "# create model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(768, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "n_epochs = 1\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "model.train()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for batch in loader:\n",
        "        audio_annotation, audio_batch, label_batch = batch\n",
        "\n",
        "        outmap_min = torch.min(label_batch)\n",
        "        outmap_max = torch.max(label_batch)\n",
        "        label_batch = (label_batch - outmap_min) / (outmap_max - outmap_min)\n",
        "\n",
        "        print(audio_batch.shape)\n",
        "        print('Label batch', label_batch)\n",
        "        y_pred = model(audio_batch) #16 X 768  4X4\n",
        "\n",
        "        y_pred = y_pred.reshape([4,1]).to(torch.float).flatten()\n",
        "        label_batch = label_batch.reshape([4,1]).to(torch.float).flatten()\n",
        "        print('y_pred', y_pred)\n",
        "\n",
        "        loss = loss_fn(y_pred, label_batch)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# create one test tensor from the testset\n",
        "annotated_data, X_test, y_test = default_collate(testset)\n",
        "model.eval()\n",
        "y_pred = model(X_test)\n",
        "acc = (y_pred.round() == y_test).float().mean()\n",
        "acc = float(acc)\n",
        "print(\"Model accuracy: %.2f%%\" % (acc*100))\n"
      ],
      "metadata": {
        "id": "FKVSn-p_1c4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_labels, input_shape):\n",
        "        super(ConvNet, self).__init()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(1, 256, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
        "        self.conv2 = nn.Conv1d(256, 256, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
        "        self.conv3 = nn.Conv1d(256, 128, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool3 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
        "        self.conv4 = nn.Conv1d(128, 64, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool4 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
        "        self.conv5 = nn.Conv1d(64, 32, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool5 = nn.MaxPool1d(kernel_size=5, stride=2, padding=2)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.fc1 = nn.Linear(32 * (input_shape // 32), 32)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(32, num_labels)\n",
        "\n",
        "        # f;\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(torch.relu(self.conv1(x)))\n",
        "        x = self.pool2(torch.relu(self.conv2(x)))\n",
        "        x = self.pool3(torch.relu(self.conv3(x)))\n",
        "        x = self.pool4(torch.relu(self.conv4(x)))\n",
        "        x = self.pool5(torch.relu(self.conv5(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "n_epochs = 1\n",
        "for epoch in range(n_epochs):\n",
        "    for batch in loader:\n",
        "        audio_annotation, audio_batch, label_batch = batch\n",
        "\n",
        "        num_labels = 1  # Number of output classes\n",
        "        input_shape = 768  # Size of the input data\n",
        "        model = ConvNet(num_labels, input_shape)\n",
        "        output = model(input_data)\n",
        "\n",
        "        # print(audio_batch.shape)\n",
        "        # print('Label batch', label_batch)\n",
        "        # y_pred = model(audio_batch) #16 X 768  4X4\n",
        "\n",
        "        # y_pred = y_pred.reshape([4,1]).to(torch.float).flatten()\n",
        "        # label_batch = label_batch.reshape([4,1]).to(torch.float).flatten()\n",
        "        # print('y_pred', y_pred)\n",
        "\n",
        "        # loss = loss_fn(y_pred, label_batch)\n",
        "        # optimizer.zero_grad()\n",
        "        # loss.backward()\n",
        "        # optimizer.step()"
      ],
      "metadata": {
        "id": "HtjzvvKAgmuy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}